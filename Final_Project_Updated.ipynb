{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitShekikyan/ADS-505-Final-Team-Project/blob/main/Final_Project_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credit Card Fraud Detection Project**\n",
        "\n",
        "## **Business Problem**\n",
        "The objective of this project is to identify fraudulent credit card transactions. Credit card fraud detection is critical for financial institutions, as it helps prevent financial loss and maintain customer trust. We aim to build a model that can accurately detect fraudulent transactions, which account for only 0.17% of all transactions in our dataset.\n",
        "\n",
        "## **Dataset Information**\n",
        "The dataset contains 284,807 transactions, with 31 features describing each transaction. The target variable indicates whether the transaction is fraudulent (1) or legitimate (0). Given the class imbalance, specific techniques will be applied to address this challenge.\n",
        "    "
      ],
      "metadata": {
        "id": "_RUuTyQtutGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Data & Libraries**"
      ],
      "metadata": {
        "id": "mwVAS-3tjzRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install dmba\n",
        "\n",
        "#library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.decomposition import PCA\n",
        "from dmba import classificationSummary\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UtSI9IjukFQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "lEl8FL_83KZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset in Google Drive\n",
        "data = '/content/drive/MyDrive/Jason Documents/creditcard.csv'\n",
        "\n",
        "# Load the CSV into a pandas DataFrame\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WS37yPJE4RO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Data Information**"
      ],
      "metadata": {
        "id": "xx1ioaVct-7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZLqvdafsATA"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there are there duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZHnwLtcf4q87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the class distribution to visualize the class imbalance\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Distribution of Legitimate vs Fraudulent Transactions')\n",
        "plt.show()\n",
        "\n",
        "# Cheking if there is there class imbalance?\n",
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "AU0-mo4XwpXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram further confirms the extreme imbalance in the dataset with very few fraudulent instances compared to the legitimate ones. We need to handle the class imbalance and do dditional exploratory data analysis focusing on the minority class (fraud) to extract more relevant insights or patterns specific to those cases.\n"
      ],
      "metadata": {
        "id": "jr2lUDGihHy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the transaction amounts for fraud vs non-fraud\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "\n",
        "# Limiting y-axis to focus on smaller amounts for clearer visualization\n",
        "plt.ylim(0, 500)\n",
        "plt.title('Transaction Amounts by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Q_fzAT3wpTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot indicates that fraudulent transactions (Class 1) have higher median transaction amounts compared to legitimate transactions (Class 0). However, both classes show a wide range of transaction amounts, and there are more outliers in the legitimate transactions, so we need to use the \"Amount\" feature as a predictor since there appears to be a noticeable difference between classes. Investigate the outliers in the legitimate transactions to determine if they might be misclassified frauds or anomalies that need special handling, and normalize or standardize the \"Amount\" variable since it may vary significantly and could influence model performance if left unscaled."
      ],
      "metadata": {
        "id": "qSAcDb3rU-C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Quality Report**\n"
      ],
      "metadata": {
        "id": "tCBi3qisRVJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary stats\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "yvaXCs9twpQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_quality_report(df):\n",
        "\n",
        "    # Initializing the report dictionary\n",
        "    report = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    # Data types\n",
        "    report['Data Type'] = df.dtypes\n",
        "\n",
        "    # Counting missing values\n",
        "    report['Missing Values'] = df.isnull().sum()\n",
        "\n",
        "    # Counting percentage of missing values\n",
        "    report['% Missing'] = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Counting of unique values\n",
        "    report['Unique Values'] = df.nunique()\n",
        "\n",
        "    # Continuous features summary (only for float64 and int64)\n",
        "    report['Min'] = df.min()\n",
        "    report['Max'] = df.max()\n",
        "    report['Mean'] = df.mean()\n",
        "    report['Median'] = df.median()\n",
        "    report['Standard Deviation'] = df.std()\n",
        "\n",
        "    # Checking for duplicates\n",
        "    report['Duplicates'] = df.duplicated().sum()\n",
        "\n",
        "    # Determine cardinality for categorical variables (assumes non-continuous variables)\n",
        "    report['Cardinality'] = [df[col].nunique() if df[col].dtype == 'object' else 'N/A' for col in df.columns]\n",
        "\n",
        "    # Return the report\n",
        "    return report\n",
        "\n",
        "# Generating the Data Quality Report for the dataset\n",
        "report = data_quality_report(df)\n",
        "\n",
        "# Display the report\n",
        "report"
      ],
      "metadata": {
        "id": "Zw2I3hPqxnLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a detailed summary of each feature in the dataset, covering various statistical and descriptive properties that are crucial for data exploration and preparation.\n",
        "\n",
        "\n",
        "*   **Data Type:** All features, except the target variable (Class), are continuous and represented as floats.\n",
        "\n",
        "*   **Missing Values:** There are no missing values, which indicates the dataset is complete and requires no imputation.\n",
        "\n",
        "\n",
        "*   **Unique Values:** The number of distinct values in each feature. High cardinality (e.g., for Time and most other features) indicates a diverse set of values, typical for continuous variables.\n",
        "\n",
        "\n",
        "*   **Min/Max:** It provides insight into the range of each feature. For example, the feature Amount ranges from 0 to 25691.16, which is consistent with transaction values.\n",
        "\n",
        "*   **Mean/Median:**  This show that most features have means close to zero. This suggests that the features might have been transformed (e.g., PCA) to center their distributions.\n",
        "\n",
        "\n",
        "*   **Standard Deviation:** It show the spread or dispersion of values within each feature. Features like \"Amount\" have a high standard deviation (250.12), reflecting a wide range of transaction values.\n",
        "\n",
        "*   **Duplicates:** There are 1081 duplicate rows in the dataset, which may need to be removed or might prevent bias in model training.\n",
        "\n",
        "\n",
        "*   **Cardinality:** Not applicable for continuous features.\n",
        "\n",
        "Based on the data quality report we need to remove duplicates, handle outliers, scale the data, and  reduce multicollinearity."
      ],
      "metadata": {
        "id": "geHcd7ma2V-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**\n",
        "\n",
        "**For Continuous Features**"
      ],
      "metadata": {
        "id": "u5H50WZZe3Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating summary statistics for continuous variables\n",
        "continuous_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "print(\"Summary statistics for continuous features:\")\n",
        "print(df[continuous_features].describe())\n",
        "\n",
        "# Visualizing continuous variables in sets of 5 per row\n",
        "n_features = len(continuous_features)\n",
        "n_cols = 5  # Number of plots per row\n",
        "\n",
        "for i in range(0, n_features, n_cols):\n",
        "    plt.figure(figsize=(15, 4))  # Adjust the width for 3 plots per row\n",
        "    for j in range(n_cols):\n",
        "        if i + j < n_features:\n",
        "            feature = continuous_features[i + j]\n",
        "\n",
        "            # Create subplot for histogram and boxplot\n",
        "            plt.subplot(1, n_cols, j + 1)\n",
        "            sns.histplot(df[feature], bins=30, kde=True)\n",
        "            plt.title(f'Distribution of {feature}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout for better spacing\n",
        "    plt.show()\n",
        "\n",
        "    # Second row with boxplots for the same features\n",
        "    plt.figure(figsize=(15, 4))  # Separate figure for boxplots\n",
        "    for j in range(n_cols):\n",
        "        if i + j < n_features:\n",
        "            feature = continuous_features[i + j]\n",
        "\n",
        "            # Create subplot for boxplot\n",
        "            plt.subplot(1, n_cols, j + 1)\n",
        "            sns.boxplot(x=df[feature])\n",
        "            plt.title(f'Boxplot of {feature}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout for better spacing\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7kLUhYYsxufI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a histograms and boxplots for each continuous feature to explore their distributions.\n",
        "\n",
        "Histograms show the distribution of data within each feature. It helps identify skewness, modality (e.g., unimodal, bimodal), and if the data follows a normal distribution. For an example features like V1, V2, and others often show normal-like distributions centered around zero. This indicates they may have been scaled or transformed.\n",
        "\n",
        "Boxplots display the spread and presence of outliers for each feature, and gave a view of the median, quartiles, and extreme values. For an exzamle of boxplots of features like V5, V6, etc., reveal many outliers, which can be important when deciding on preprocessing techniques, like applying robust scaling or addressing extreme values to avoid undue influence on the models.\n",
        "\n",
        "Both histograms and boxplots are displayed side by side for each set of features, ensuring that the visual exploration of the dataset is comprehensive."
      ],
      "metadata": {
        "id": "40zMw-AHzjxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Categorical Features**"
      ],
      "metadata": {
        "id": "Z0WmUH1NyFBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the target variable 'Class'\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Distribution of Class (Fraud vs Non-Fraud)')\n",
        "plt.show()\n",
        "\n",
        "# Display percentage of fraud vs non-fraud transactions\n",
        "class_counts = df['Class'].value_counts(normalize=True) * 100\n",
        "print(\"Percentage distribution of the target variable 'Class':\")\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "ldnzqRM-yGRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart displays the distribution of the target variable, Class, which represents legitimate transactions (Class 0) and fraudulent transactions (Class 1). It shows a significant imbalance between the two classes, with legitimate transactions making up 99.83% of the dataset, while fraudulent transactions only constitute 0.17%.\n",
        "\n",
        "The extreme class imbalance indicated by the chart requires special consideration in the modeling process to ensure the model's effectiveness in identifying the minority class (fraudulent transactions). Proper resampling, algorithm adjustments, and appropriate evaluation metrics will be crucial to developing an effective fraud detection model.\n"
      ],
      "metadata": {
        "id": "HAcTBk_u9GQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Analysis**"
      ],
      "metadata": {
        "id": "dC1dUaPfydoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a subset of features to avoid too many plots (e.g., V1 to V5)\n",
        "subset_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Class']\n",
        "\n",
        "# Creating a pair plot\n",
        "sns.pairplot(df[subset_features], hue='Class', diag_kind='kde', plot_kws={'alpha': 0.3})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_qnN83sygXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot shows the relationships and scatter distributions of selected features (e.g., V1, V2, etc.) across different classes. There are clusters and some separable patterns, particularly between legitimate and fraudulent classes. Here we need to identify and prioritize features that show clear separability between classes as important predictors for modeling. We might use the PCA if multicollinearity or highly correlated features are identified, and clustering algorithms to validate whether the clusters align with the fraud and non-fraud classes."
      ],
      "metadata": {
        "id": "wG-bSwP4jL6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Ploting the heatmap of the correlation matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pglbuQeUyoi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap illustrates the correlation between different features. Some features show strong correlations, which could indicate redundancy or multicollinearity issues. Here are the steps we might go for the next steps:\n",
        "\n",
        "\n",
        "*   Removing or combine highly correlated features to avoid multicollinearity in model building (e.g., using PCA or dropping one of the correlated features).\n",
        "\n",
        "*   Focusing on features with stronger correlations with the target variable (Class) as they might be more predictive."
      ],
      "metadata": {
        "id": "_f7R-44Rktk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Violin plot for 'Amount' based on 'Class'\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.violinplot(x='Class', y='Amount', data=df)\n",
        "plt.title('Distribution of Transaction Amounts by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WTjlii_Tyufl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot gives a deeper look into the distribution and density of transaction amounts by class. It shows that fraudulent transactions have a more concentrated distribution compared to legitimate ones, which are more spread out with outliers. Here we can explore additional features that might help differentiate frauds based on amount distributions, also handling outliers in the legitimate transactions to refine model performance."
      ],
      "metadata": {
        "id": "Dx_BwX2alZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keeping numeric features (skip target 'Class')\n",
        "X = df.drop(columns=['Class'])\n",
        "\n",
        "# Calculating VIF (Variance Inflation Factor) for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "guC9NDxgy1Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VIF output indicates the level of multicollinearity among the features in dataset.\n",
        "\n",
        "Most features (e.g., \"V1\", \"V3\", \"V6\", \"V8\", and others) have VIF values close to 1, indicating very low multicollinearity. This suggests that these features are not significantly correlated with other features in the dataset, making them reliable for use in regression models.\n",
        "\n",
        "Features like \"V2\", \"V5\", \"V7\", and \"V20\" have VIF values between 2 and 4. These values are still within an acceptable range but indicate some degree of correlation with other features. While these values do not warrant immediate removal, it's important to monitor these features in case they lead to multicollinearity issues in your model.\n",
        "\n",
        "The \"Amount\" feature has a VIF value of approximately 11.5, which is quite high and indicates significant multicollinearity. This suggests that \"Amount\" may be highly correlated with one or more other features in the dataset. High VIF values like this could distort regression coefficients and affect the stability and interpretability of the model.\n",
        "\n",
        "Since Amount has a high VIF value, you might need to explore its correlation with other features. If it's highly correlated with other variables, we can removing it, if it doesn't contribute new information or combine it with other features or transform it to reduce the correlation.\n",
        "\n",
        "Although the VIF values between 2 and 4 are acceptable, it's a good idea to keep these features in mind when evaluating model performance, as they might introduce minor multicollinearity."
      ],
      "metadata": {
        "id": "jpby661FMgQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'Class' and compute summary statistics for continuous features\n",
        "grouped_stats = df.groupby('Class').mean()\n",
        "print(grouped_stats)"
      ],
      "metadata": {
        "id": "QKWsDl5xy_eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot of Amount grouped by Class\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "plt.title('Transaction Amount by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aAqkjRJWzJWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This boxplot visualizes the distribution of transaction amounts for legitimate (Class 0) and fraudulent (Class 1) transactions. Fegitimate transactions (Class 0), the majority of transaction amounts are concentrated at lower values, with a few extreme outliers reaching up to over 25,000 and but  presence of numerous outliers suggests that legitimate transactions vary widely in amount, with most being small but some being very large.\n",
        "\n",
        "For fraudlent transactions (Class 1) show low fraudulent transaction but appear to be more tightly distributed, with significantly fewer extreme outliers compared to legitimate transactions. Which suggests that fraudulent transactions generally tend to have lower amounts, and there is less variation in their values."
      ],
      "metadata": {
        "id": "9QB_rGTU7Wpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "In this section, we will handle missing values, scale the data, and address class imbalance."
      ],
      "metadata": {
        "id": "KJphfztqvc3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "8l8ZLpT_N5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there are there still any duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "76QUuZLz7iSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains substantial outliers, which is common in fraud detection, so RobustScaler is primary choice, because this scaler centers the data using the median and scales using the interquartile range (IQR), which makes it robust to outliers. So, to ensure that outliers are minimized first we will use  RobustScaler and then further normalize the data for algorithms sensitive to scaling StandardScaler.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u2IQDwKqFcqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a correlation matrix with imbalanced and SubSample data\n",
        "fraud = df[df['Class'] == 1]\n",
        "non_fraud = df[df['Class'] == 0].sample(n=len(fraud), random_state=42)\n",
        "\n",
        "# Concatenate fraud and non-fraud samples to create a balanced subsample\n",
        "subsample = pd.concat([fraud, non_fraud])\n",
        "\n",
        "# Compute correlation matrices\n",
        "corr_matrix_full = df.corr()\n",
        "corr_matrix_subsample = subsample.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n",
        "\n",
        "# Imbalanced correlation matrix\n",
        "sns.heatmap(corr_matrix_full, cmap='coolwarm', ax=ax[0], cbar_kws={'shrink': 0.5}, vmin=-1, vmax=1)\n",
        "ax[0].set_title('Imbalanced Correlation Matrix\\n(don\\'t use for reference)', fontsize=16)\n",
        "\n",
        "# Subsample correlation matrix\n",
        "sns.heatmap(corr_matrix_subsample, cmap='coolwarm', ax=ax[1], cbar_kws={'shrink': 0.5}, vmin=-1, vmax=1)\n",
        "ax[1].set_title('SubSample Correlation Matrix\\n(use for reference)', fontsize=6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6XWNsAD6v6bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two correlation matrices illustrate the relationship between different features in the dataset, showing how preprocessing, especially subsampling, impacts the representation and correlations between features.\n",
        "\n",
        "Imbalanced correlaition matrix show the  lacks variability and detail, indicating that the class imbalance masks true relationships between features. This is because the overwhelming majority of non-fraudulent (Class 0) data points dominate the calculation, making the correlation values less reliable for model development. This matrix is not ok to  be used as a reference for feature selection or understanding relationships since it is skewed by the imbalance.\n",
        "\n",
        "SubSample correlation matrix shows a more diverse pattern of correlations, with a mixture of positive and negative correlations across different feature pairs. This suggests that the subsampled dataset is better balanced, allowing the true relationships to surface. There is a\n",
        "strong correlations between some features (e.g., between V3, V6, and V9) which could indicate multicollinearity. These features might need to be addressed using techniques like Principal Component Analysis (PCA) or feature elimination to reduce redundancy.\n",
        "\n"
      ],
      "metadata": {
        "id": "p0OvA9b3A3Gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Algorithm"
      ],
      "metadata": {
        "id": "iKRE89CGDVDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model 1 _ Logistic Regression"
      ],
      "metadata": {
        "id": "dEhwsmMoRb6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Re assigning X after removing duplicates from df\n",
        "X = df.drop(columns=['Class'])\n",
        "\n",
        "# Applying RobustScaler first\n",
        "# Will be using X which was defined earlier section\n",
        "robust_scaler = RobustScaler()\n",
        "X_robust_scaled = robust_scaler.fit_transform(X)\n",
        "\n",
        "# Applying StandardScaler on the robust-scaled data\n",
        "standard_scaler = StandardScaler()\n",
        "X_final_scaled = standard_scaler.fit_transform(X_robust_scaled)"
      ],
      "metadata": {
        "id": "dpOO30qCFN4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are setting 'y' as the target variable, df is the original dataset\n",
        "y = df['Class']\n",
        "\n",
        "# Spliting the dataset into training and testing sets using X_final_scaled that was created earlier\n",
        "# Here we will be using stratiy during train test split due to the imbalanced nature of the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Initializing the logistic regression model, adjusting max_iter if necessary for convergence\n",
        "# Here we are applying class_weight as balanced to automatically assigns weights inversely proportional to the class frequencies in the data\n",
        "log_reg = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
        "\n",
        "# Training the model using the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "y_pred_prob_lr = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluating the model\n",
        "# We will be skipping the accuracy since in imbalanced dataset, it may not be meaningful\n",
        "# Instead, we will focus more on Precision, Recall, and AUPRC\n",
        "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_pred_prob_lr)\n",
        "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "# Calculating AUPRC\n",
        "auprc_lr = auc(recall_lr, precision_lr)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_lr)\n",
        "print(\"\\nClassification Report:\\n\", report_lr)\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc_lr:.2f}\")"
      ],
      "metadata": {
        "id": "c4liHU_kajuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 _ Random Forest Classifier"
      ],
      "metadata": {
        "id": "_8g5nULQRg6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomForestClassifier with class weights to handle imbalance\n",
        "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluating the model\n",
        "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_pred_prob_rf)\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Calculating AUPRC\n",
        "auprc_rf = auc(recall_rf, precision_rf)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_rf)\n",
        "print(\"\\nClassification Report:\\n\", report_rf)\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc_rf:.2f}\")"
      ],
      "metadata": {
        "id": "ZLLD1zVoy_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access one of the trees in the forest (e.g., the first tree)\n",
        "tree = rf_model.estimators_[0]  # Accessing the first tree in the forest\n",
        "\n",
        "# Visualizing the tree\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_tree(tree,\n",
        "          filled=True,\n",
        "          feature_names=X_train.columns if isinstance(X_train, pd.DataFrame) else ['Feature_' + str(i) for i in range(X_train.shape[1])],\n",
        "          class_names=['Not Fraud', 'Fraud'],\n",
        "          rounded=True)\n",
        "plt.title('Decision Tree Visualization (Tree 1 of Random Forest)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HXJdIACujSt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree visualization provided represents one of the individual trees from the Random Forest model built to classify transactions as either fraudulent or non-fraudulent. The tree is composed of several levels where splits are made based on specific feature values, with each node representing a decision point. The tree uses these splits to divide the dataset into smaller subsets, attempting to group similar observations together. The decision rules at each node are determined based on features that maximize information gain, measured by a reduction in Gini impurity, which indicates how mixed the classes are within a node. A Gini value of 0 at a node means that all samples belong to a single class, making the node pure.\n",
        "\n",
        "As the tree progresses down each path, it uses different features, such as Feature_1, Feature_14, and Feature_12, suggesting that these attributes are influential in distinguishing between fraudulent and non-fraudulent transactions. Early splits in the tree rely on these critical features, helping the model make initial decisions about which direction the transaction should proceed. The tree then continues to branch out further using other features, each split further refining the classification until the leaf nodes are reached. These leaf nodes represent the final classification for the observations, where the \"class\" value shows whether the majority of samples at that node are classified as fraud or not fraud. It’s notable that some branches become pure (Gini = 0) quite quickly, indicating a clear separation of classes based on certain feature values, while other branches require more depth to achieve purity.\n",
        "\n",
        "This individual tree is just one part of the entire Random Forest, which is composed of many such trees, each capturing different aspects of the data. While this single tree provides insight into how certain features and their values influence the classification process, it is important to remember that the forest, as a whole, averages the predictions from all these trees, reducing the risk of overfitting. Overfitting is a concern when trees grow too deep and fit noise in the training data rather than capturing general patterns. However, the Random Forest model mitigates this by aggregating results across multiple trees, leading to a more robust and generalizable model. This visualization allows us to understand the decision-making process within the forest and highlights the importance of certain features in classifying fraudulent versus non-fraudulent transactions."
      ],
      "metadata": {
        "id": "2QlAWN_7zwKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Algorithm"
      ],
      "metadata": {
        "id": "qwp9MiyTK8_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model 3 _ Isolation Forest"
      ],
      "metadata": {
        "id": "tBlUvWTZRoKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Isolation Forest model\n",
        "# Here we use a high contamination of 0.05 to try catch more fraud transactions\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "\n",
        "# Fit the model using the X_final_scaled from before\n",
        "isolation_forest.fit(X_final_scaled)\n",
        "\n",
        "# Predict anomalies using the Isolation Forest model\n",
        "# The predict method returns -1 for anomalies and 1 for normal data\n",
        "y_pred_if = isolation_forest.predict(X_final_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and 1 (normal) to 0 (non-fraud)\n",
        "y_pred_converted_if = [1 if x == -1 else 0 for x in y_pred_if]\n",
        "\n",
        "# Evaluating the model\n",
        "conf_matrix_if = confusion_matrix(y, y_pred_converted_if)\n",
        "report_if = classification_report(y, y_pred_converted_if)\n",
        "\n",
        "# Step 1: Get anomaly scores from the Isolation Forest model\n",
        "# The decision_function method provides the anomaly scores (higher score indicates a normal point, lower score indicates an outlier)\n",
        "anomaly_scores = isolation_forest.decision_function(X_final_scaled)\n",
        "\n",
        "# Step 2: Calculate precision, recall, and thresholds using precision_recall_curve\n",
        "# Use negative scores to match with anomaly prediction\n",
        "precision_if, recall_if, thresholds = precision_recall_curve(y, -anomaly_scores)\n",
        "\n",
        "# Step 3: Calculate the AUPRC\n",
        "auprc_if = auc(recall_if, precision_if)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_if)\n",
        "print(\"\\nClassification Report:\\n\", report_if)\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc_if:.2f}\")"
      ],
      "metadata": {
        "id": "Xp6S7TdloyW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model 4 _ DBSCAN"
      ],
      "metadata": {
        "id": "2Z6n-SwlBCz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DBSCAN model\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can tune these hyperparameters\n",
        "\n",
        "# Here we would fit and predict directly\n",
        "y_preds_dbscan = dbscan.fit_predict(X_final_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and others to 0 (non-fraud)\n",
        "y_preds_convert_dbscan = [1 if x == -1 else 0 for x in y_preds_dbscan]\n",
        "\n",
        "# Evaluating the model\n",
        "conf_matrix_dbscan = confusion_matrix(y, y_preds_convert_dbscan)\n",
        "report_dbscan = classification_report(y, y_preds_convert_dbscan)\n",
        "\n",
        "# Calculate precision, recall, and thresholds using precision_recall_curve\n",
        "precision_dbscan, recall_dbscan, thresholds_dbscan = precision_recall_curve(y, y_preds_convert_dbscan)\n",
        "\n",
        "# Calculate the AUPRC\n",
        "auprc_dbscan = auc(recall_dbscan, precision_dbscan)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix_dbscan)\n",
        "print(\"\\nClassification Report:\\n\", report_dbscan)\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc_dbscan:.2f}\")"
      ],
      "metadata": {
        "id": "udzUC-mD7_E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Area Under the Precision-Recall Curve for All The Models"
      ],
      "metadata": {
        "id": "qPTFhHueHtlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plotting Precision-Recall Curve for Logistic Regression\n",
        "plt.plot(recall_lr, precision_lr, label=f'Logistic Regression (AUPRC = {auprc_lr:.2f})')\n",
        "\n",
        "# Plotting Precision-Recall Curve for Random Forest\n",
        "plt.plot(recall_rf, precision_rf, label=f'Random Forest (AUPRC = {auprc_rf:.2f})')\n",
        "\n",
        "# Plotting Precision-Recall Curve for Isolation Forest\n",
        "plt.plot(recall_if, precision_if, label=f'Isolation Forest (AUPRC = {auprc_if:.2f})')\n",
        "\n",
        "# Plotting Precision-Recall Curve for DBSCAN\n",
        "plt.plot(recall_dbscan, precision_dbscan, label=f'DBSCAN (AUPRC = {auprc_dbscan:.2f})')\n",
        "\n",
        "# Adding labels, title, legend, and grid\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Multiple Models')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0VF92bT2GpsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating The Models"
      ],
      "metadata": {
        "id": "UXIZizjEI_9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty list to store results\n",
        "results = []\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': y_pred_lr,\n",
        "    'Random Forest': y_pred_rf,\n",
        "    'Isolation Forest': y_pred_converted_if,\n",
        "    'DBSCAN': y_preds_convert_dbscan\n",
        "}\n",
        "\n",
        "# Training and evaluating each model\n",
        "for model_name, model in models.items():\n",
        "\n",
        "    # Calculating metrics\n",
        "    if (model_name == 'Logistic Regression') or (model_name == 'Random Forest'):\n",
        "      precision = precision_score(y_test, model)\n",
        "      recall = recall_score(y_test, model)\n",
        "    else:\n",
        "      precision = precision_score(y, model)\n",
        "      recall = recall_score(y, model)\n",
        "\n",
        "    # Append results to the list\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall\n",
        "    })\n",
        "\n",
        "# Converting results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Displaying the results\n",
        "display(df_results)"
      ],
      "metadata": {
        "id": "I2HSO8RFJvs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "GunXsokjIjKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Logistic Regression (AUPRC = 0.70):** This model performs moderately, with an AUPRC (Area Under Precision-Recall Curve) of 70%. However, it has a very bad precision. This is understandable due to the nature of the imbalanced dataset. This is very optimal, however, we have to be aware that the dataset has been stratify during train test split and have a smaller overall dataset.\n",
        "\n",
        "**Random Forest (AUPRC = 0.80):** The Random Forest model achieves a very good AUC of 80%, which indicates good separation between classes. The AUPRC curve follows the axes precisely, meaning it correctly identifies most of the true positives and negatives without any error. Surprisingly, it also has a very good precision and a decent recall. This is very optimal, however, we have to be aware that the dataset has been stratify during train test split and have a smaller overall dataset.\n",
        "\n",
        "**Isolation Forest (AUPRC = 0.09):** Isolation Forest is obviously not performing well with a AUPRC of only 9%. This indicates that it is even worse than random guessing of 50/50. Although we are using the entire dataset without stratifying it, it is still performing very poorly\n",
        "\n",
        "**DBSCAN (AUPRC = 0.50):** The DBSCAN (Density-based spatial clustering of applications with noise) model, with an AUPRC of 50%, indicates that it is only perfoming just as good as random guess. Overall in general, it seems like the unsupervised Algorithm is not performing well."
      ],
      "metadata": {
        "id": "D0LYJNw8MJvm"
      }
    }
  ]
}